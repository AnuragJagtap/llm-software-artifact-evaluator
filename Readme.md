# LLM Software Artifact Evaluator

## Overview
This project presents a unified evaluation framework for assessing **software engineering artifacts generated by Large Language Models (LLMs)** across multiple stages of the Software Development Life Cycle (SDLC).

The system evaluates:
- **Software Requirement Specification (SRS) documents**
- **Source code generated by LLMs**
- **Test cases generated by LLMs**

Using **GEval-inspired evaluation metrics**, the framework produces:
- Metric-wise scores
- An overall quality score
- Short qualitative explanations justifying each evaluation

The project is implemented in **Python** and demonstrated using **Jupyter Notebooks**, with full support for both **Google Colab** and **local execution**.

---

## Key Features
- Unified evaluation of multiple LLM-generated software artifacts
- GEval-style metric framework for qualitative and quantitative assessment
- Artifact-wise and metric-wise scoring
- Automated natural language explanations
- Modular and extensible design
- Reproducible execution using notebooks
- Compatible with Google Colab and local environments

---

## Artifacts Evaluated

| Artifact Type | Evaluation Focus |
|--------------|------------------|
| SRS Document | Completeness, clarity, consistency, correctness |
| Source Code | Functional correctness, execution validity, efficiency, readability |
| Test Cases  | Coverage, relevance, edge case handling, effectiveness |

---

