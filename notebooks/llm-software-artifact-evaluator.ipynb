{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6psdyU7xQMOn",
        "outputId": "6e641972-12c0-49eb-e456-991e755ec08d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit\n",
            "  Downloading streamlit-1.52.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Collecting pdfminer.six==20251230 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.4.59)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.52.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pydeck, pdfminer.six, pdfplumber, streamlit\n",
            "Successfully installed pdfminer.six-20251230 pdfplumber-0.11.9 pydeck-0.9.1 pypdfium2-5.3.0 streamlit-1.52.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain tiktoken pdfplumber streamlit google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fSD_e2gNfCCR",
        "outputId": "8953fdd5-28c1-498b-8c0a-279473163fc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.12.0)\n",
            "Collecting openai\n",
            "  Downloading openai-2.15.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Downloading openai-2.15.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.12.0\n",
            "    Uninstalling openai-2.12.0:\n",
            "      Successfully uninstalled openai-2.12.0\n",
            "Successfully installed openai-2.15.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yjq3_TwDQZyo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"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\",\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx2xtxlKD_dJ"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENROUTER_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKNerQMH9ytX"
      },
      "outputs": [],
      "source": [
        "def load_file(path):\n",
        "    with open(path, \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "problem_description = load_file(\"/content/problem_description.txt\")\n",
        "generated_code = load_file(\"/content/sample_code.java\")\n",
        "test_cases = load_file(\"/content/testcases.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93yTW6_ASBp7"
      },
      "source": [
        "SRS Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgn_UThhSBU4"
      },
      "outputs": [],
      "source": [
        "GEVAL_METRICS_SRS = [\n",
        "    {\n",
        "        \"name\": \"Requirements Completeness\",\n",
        "        \"description\": (\n",
        "            \"Does the SRS comprehensively cover all necessary functional and non-functional \"\n",
        "            \"requirements, including system features, constraints, assumptions, and dependencies?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Clarity & Unambiguity\",\n",
        "        \"description\": (\n",
        "            \"Are the requirements clearly stated, precise, and free from ambiguity, vague terms, \"\n",
        "            \"or subjective language that could lead to multiple interpretations?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Consistency\",\n",
        "        \"description\": (\n",
        "            \"Are there any conflicting, duplicated, or contradictory requirements within the SRS, \"\n",
        "            \"or between functional and non-functional sections?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Verifiability & Testability\",\n",
        "        \"description\": (\n",
        "            \"Can each requirement be objectively verified or tested through inspection, analysis, \"\n",
        "            \"demonstration, or test cases?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Structure & Standard Compliance\",\n",
        "        \"description\": (\n",
        "            \"Is the SRS well-structured and logically organized, following standard SRS formats \"\n",
        "            \"(e.g., IEEE 830 / IEEE 29148) with proper sections and hierarchy?\"\n",
        "        )\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dHjd9L_SFu7"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "srs_text = extract_text_from_pdf(\"generated_srs.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWMZ_cudTB4-"
      },
      "outputs": [],
      "source": [
        "def build_geval_prompt_srs_pdf(srs_pdf_text, metrics):\n",
        "    metric_text = \"\\n\".join([\n",
        "        f\"{i+1}. {m['name']}: {m['description']}\"\n",
        "        for i, m in enumerate(metrics)\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "You are an expert software requirements analyst and SRS reviewer.\n",
        "\n",
        "Evaluate the following Software Requirements Specification (SRS) document\n",
        "generated by a Large Language Model. The SRS content has been extracted\n",
        "from a PDF file.\n",
        "\n",
        "SRS DOCUMENT (Extracted from PDF):\n",
        "{srs_pdf_text}\n",
        "\n",
        "EVALUATION METRICS:\n",
        "{metric_text}\n",
        "\n",
        "For each metric:\n",
        "- Assign a score from 1 to 10 (higher is better).\n",
        "- Provide a concise and technically sound justification.\n",
        "- Evaluate strictly based on SRS best practices and standards\n",
        "  (e.g., IEEE 830 / IEEE 29148).\n",
        "\n",
        "Return STRICTLY in valid JSON format.\n",
        "Do NOT include markdown, explanations, or additional text.\n",
        "\n",
        "Expected JSON format:\n",
        "{{\n",
        "  \"Metric Name\": {{\n",
        "      \"score\": <int>,\n",
        "      \"reason\": \"<short justification>\"\n",
        "  }},\n",
        "  ...\n",
        "  \"Overall Score\": <float>\n",
        "}}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwSuQYbcTGpf"
      },
      "outputs": [],
      "source": [
        "def run_geval_srs(prompt):\n",
        "    \"\"\"\n",
        "    Executes GEVAL evaluation for an SRS document (text extracted from PDF).\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Fully constructed GEVAL prompt for SRS evaluation.\n",
        "\n",
        "    Returns:\n",
        "        str: Raw JSON response from the LLM evaluator.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=900\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_P7IO9-TeKv",
        "outputId": "5bff312e-8c7a-4e95-e147-fe7ee21b1fc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the evaluation of the SRS document in JSON format:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"Requirements Completeness\": {\n",
            "    \"score\": 6,\n",
            "    \"reason\": \"The SRS covers the main functional and non-functional requirements, but lacks details on book management, user authentication, and error handling.\"\n",
            "  },\n",
            "  \"Clarity & Unambiguity\": {\n",
            "    \"score\": 8,\n",
            "    \"reason\": \"The requirements are generally clear and concise, but some terms like 'unique Book ID' could be clarified.\"\n",
            "  },\n",
            "  \"Consistency\": {\n",
            "    \"score\": 9,\n",
            "    \"reason\": \"The SRS is well-organized and consistent in its structure, but there is a minor inconsistency in the use of 'shall' and 'should' in the functional and non-functional requirements.\"\n",
            "  },\n",
            "  \"Verifiability & Testability\": {\n",
            "    \"score\": 7,\n",
            "    \"reason\": \"Most requirements can be objectively verified or tested, but some, like NFR1, are too vague to be tested directly.\"\n",
            "  },\n",
            "  \"Structure & Standard Compliance\": {\n",
            "    \"score\": 5,\n",
            "    \"reason\": \"The SRS does not follow the standard IEEE 830/IEEE 29148 format, and some sections (e.g., Assumptions) are not clearly defined.\"\n",
            "  },\n",
            "  \"Overall Score\": 6.8\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Extract text from the SRS PDF\n",
        "srs_pdf_text = extract_text_from_pdf(\"/content/generated_srs.pdf\")\n",
        "\n",
        "# Step 2: Build GEVAL prompt for SRS evaluation\n",
        "prompt_srs = build_geval_prompt_srs_pdf(\n",
        "    srs_pdf_text,\n",
        "    GEVAL_METRICS_SRS\n",
        ")\n",
        "\n",
        "raw_output_srs = run_geval_srs(prompt_srs)\n",
        "print(raw_output_srs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjVa_IM9sSY"
      },
      "source": [
        "Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjfUd9LB9rNR"
      },
      "outputs": [],
      "source": [
        "GEVAL_METRICS_CG = [\n",
        "    {\n",
        "        \"name\": \"Functional Correctness\",\n",
        "        \"description\": \"Does the code correctly solve the problem for all valid inputs and edge cases?\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Compilation / Execution Validity\",\n",
        "        \"description\": \"Is the code free from syntax and runtime errors?\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Algorithmic Efficiency\",\n",
        "        \"description\": \"Is the time and space complexity appropriate for the problem constraints?\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Readability & Maintainability\",\n",
        "        \"description\": \"Is the code clean, readable, and well-structured?\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Edge Case Handling\",\n",
        "        \"description\": \"Does the code correctly handle boundary and corner cases?\"\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFvCoQG5-Rvg"
      },
      "outputs": [],
      "source": [
        "def build_geval_prompt_cg(problem, code, metrics):\n",
        "    metric_text = \"\\n\".join([\n",
        "        f\"{i+1}. {m['name']}: {m['description']}\"\n",
        "        for i, m in enumerate(metrics)\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "You are an expert software engineer and code reviewer.\n",
        "\n",
        "Evaluate the following generated code using the metrics below.\n",
        "\n",
        "PROBLEM DESCRIPTION:\n",
        "{problem}\n",
        "\n",
        "GENERATED CODE:\n",
        "{code}\n",
        "\n",
        "EVALUATION METRICS:\n",
        "{metric_text}\n",
        "\n",
        "For each metric:\n",
        "- Give a score (1-10) and short reason for each metric.\n",
        "- Return ONLY valid JSON in this format:\n",
        "\n",
        "Return STRICTLY in valid JSON format:\n",
        "{{\n",
        "  \"Metric Name\": {{\n",
        "      \"score\": <int>,\n",
        "      \"reason\": \"<text>\"\n",
        "  }},\n",
        "  ...\n",
        "  \"Overall Score\": <float>\n",
        "}}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgmqm_Kh-lfK"
      },
      "outputs": [],
      "source": [
        "def run_geval_cg(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=700\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0olg5m55-rmO",
        "outputId": "63c61ad2-fd45-46dc-d0b5-358192c5ed9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the evaluation of the generated code:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"Functional Correctness\": {\n",
            "    \"score\": 8,\n",
            "    \"reason\": \"The code correctly implements the required functionality, but it does not handle the case where the number of copies is set to a negative value when adding a book.\"\n",
            "  },\n",
            "  \"Compilation / Execution Validity\": {\n",
            "    \"score\": 10,\n",
            "    \"reason\": \"The code is free from syntax and runtime errors.\"\n",
            "  },\n",
            "  \"Algorithmic Efficiency\": {\n",
            "    \"score\": 6,\n",
            "    \"reason\": \"The time complexity of the code is O(n) for the listBooks method, which is acceptable for a small library. However, for a large library, this could be improved by using a data structure that allows for faster lookup, such as a HashSet or a database.\"\n",
            "  },\n",
            "  \"Readability & Maintainability\": {\n",
            "    \"score\": 9,\n",
            "    \"reason\": \"The code is well-structured and easy to read, but it could benefit from more comments and documentation to explain the purpose of each method and the logic behind the code.\"\n",
            "  },\n",
            "  \"Edge Case Handling\": {\n",
            "    \"score\": 7,\n",
            "    \"reason\": \"The code correctly handles the case where a book with the same ID already exists, but it does not handle the case where the number of copies is set to a negative value when adding a book.\"\n",
            "  },\n",
            "  \"Overall Score\": 7.6\n",
            "}\n",
            "```\n",
            "\n",
            "Note: The overall score is a weighted average of the individual scores, with each metric weighted equally.\n"
          ]
        }
      ],
      "source": [
        "prompt_cg = build_geval_prompt_cg(problem_description, generated_code, GEVAL_METRICS_CG)\n",
        "\n",
        "raw_output_cg = run_geval_cg(prompt_cg)\n",
        "print(raw_output_cg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YheFhIxk9wYf"
      },
      "source": [
        "Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvLa127jQZvA"
      },
      "outputs": [],
      "source": [
        "GEVAL_METRICS_T = [\n",
        "    {\n",
        "        \"name\": \"Test Case Coverage\",\n",
        "        \"description\": (\n",
        "            \"Do the generated test cases adequately cover normal, boundary, and extreme input scenarios \"\n",
        "            \"defined by the problem constraints?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Edge Case Effectiveness\",\n",
        "        \"description\": (\n",
        "            \"Do the test cases include meaningful edge and corner cases that are likely to expose defects \"\n",
        "            \"in incorrect or incomplete implementations?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Input Validity & Constraints Compliance\",\n",
        "        \"description\": (\n",
        "            \"Are all test inputs valid, well-formed, and compliant with the stated input constraints \"\n",
        "            \"(data types, ranges, formats, and sizes)?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Expected Output Correctness\",\n",
        "        \"description\": (\n",
        "            \"Are the expected outputs accurate and logically consistent with the problem specification \"\n",
        "            \"for each generated test case?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Redundancy & Diversity\",\n",
        "        \"description\": (\n",
        "            \"Do the test cases avoid unnecessary duplication while maintaining sufficient diversity \"\n",
        "            \"to test different execution paths?\"\n",
        "        )\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y6RieTxQZpR"
      },
      "outputs": [],
      "source": [
        "def build_geval_prompt_tc(problem, code, testcases, metrics):\n",
        "    metric_text = \"\\n\".join([\n",
        "        f\"{i+1}. {m['name']}: {m['description']}\"\n",
        "        for i, m in enumerate(metrics)\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "You are an expert software testing engineer and quality analyst.\n",
        "\n",
        "Your task is to evaluate the quality of test cases generated by a Large Language Model (LLM).\n",
        "The goal is to assess whether these test cases are effective at validating the correctness\n",
        "and robustness of the given code with respect to the problem description.\n",
        "\n",
        "PROBLEM DESCRIPTION:\n",
        "{problem}\n",
        "\n",
        "REFERENCE CODE UNDER TEST:\n",
        "{code}\n",
        "\n",
        "GENERATED TEST CASES:\n",
        "{testcases}\n",
        "\n",
        "EVALUATION METRICS:\n",
        "{metric_text}\n",
        "\n",
        "For each metric:\n",
        "- Assign a score between 1 and 10\n",
        "- Provide a concise, technical justification focused on test effectiveness\n",
        "\n",
        "Return STRICTLY valid JSON in the following format:\n",
        "{{\n",
        "  \"Metric Name\": {{\n",
        "      \"score\": <int>,\n",
        "      \"reason\": \"<concise explanation>\"\n",
        "  }},\n",
        "  ...\n",
        "  \"Overall Score\": <float>\n",
        "}}\n",
        "\n",
        "Important:\n",
        "- Do NOT evaluate code style or performance\n",
        "- Focus ONLY on test coverage, correctness, diversity, and defect-detection capability\n",
        "- Penalize redundant, invalid, or weak test cases\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1PjL3AyQZet"
      },
      "outputs": [],
      "source": [
        "def run_testcase_geval_tc(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=700\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l85XBa-5T8lW",
        "outputId": "9c904757-d2c1-414b-ee6e-e99f3fa9a482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's the evaluation of the generated test cases based on the provided metrics:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"Test Case Coverage\": {\n",
            "    \"score\": 8,\n",
            "    \"reason\": \"The test cases cover most normal and boundary input scenarios, but lack extreme input scenarios, such as adding a book with a negative number of copies or issuing a book with a non-existent ID.\"\n",
            "  },\n",
            "  \"Edge Case Effectiveness\": {\n",
            "    \"score\": 6,\n",
            "    \"reason\": \"The test cases include some meaningful edge cases, such as adding a duplicate book and issuing a book when no copies are available, but miss others, like returning a book with no copies available.\"\n",
            "  },\n",
            "  \"Input Validity & Constraints Compliance\": {\n",
            "    \"score\": 9,\n",
            "    \"reason\": \"The test inputs are generally valid and well-formed, but the input for Test Case 4 (issueBook(999)) is not compliant with the problem constraints, as the book ID is not present in the system.\"\n",
            "  },\n",
            "  \"Expected Output Correctness\": {\n",
            "    \"score\": 9,\n",
            "    \"reason\": \"The expected outputs are mostly accurate and logically consistent with the problem specification, but Test Case 4's expected output is incorrect, as the book is not found instead of no copies available.\"\n",
            "  },\n",
            "  \"Redundancy & Diversity\": {\n",
            "    \"score\": 7,\n",
            "    \"reason\": \"The test cases avoid unnecessary duplication, but some test cases, like Test Case 1 and Test Case 2, test similar scenarios, and the test cases could benefit from more diversity to test different execution paths.\"\n",
            "  },\n",
            "  \"Overall Score\": 7.6\n",
            "}\n",
            "```\n",
            "\n",
            "The overall score is a weighted average of the individual metric scores, with each metric contributing equally to the overall score. The justification for the overall score is that the test cases demonstrate good coverage of normal and boundary input scenarios, but lack extreme input scenarios and diversity in testing different execution paths. Additionally, some test cases have minor issues with expected output correctness and input validity.\n"
          ]
        }
      ],
      "source": [
        "prompt_tc = build_geval_prompt_tc(\n",
        "    problem=problem_description,\n",
        "    code=generated_code,\n",
        "    testcases=test_cases,\n",
        "    metrics=GEVAL_METRICS_T\n",
        ")\n",
        "\n",
        "raw_output_tc = run_testcase_geval_tc(prompt_tc)\n",
        "print(raw_output_tc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xgh3NQdi2_f",
        "outputId": "a0a15e3e-cfe9-4461-f2be-3e0b8980ebff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined evaluations saved to combined_evaluations.json\n",
            "\n",
            "Content of combined_evaluations.json:\n",
            "\n",
            "{\n",
            "  \"srs_evaluation\": {\n",
            "    \"Requirements Completeness\": {\n",
            "      \"score\": 6,\n",
            "      \"reason\": \"The SRS covers the main functional and non-functional requirements, but lacks details on book management, user authentication, and error handling.\"\n",
            "    },\n",
            "    \"Clarity & Unambiguity\": {\n",
            "      \"score\": 8,\n",
            "      \"reason\": \"The requirements are generally clear and concise, but some terms like 'unique Book ID' could be clarified.\"\n",
            "    },\n",
            "    \"Consistency\": {\n",
            "      \"score\": 9,\n",
            "      \"reason\": \"The SRS is well-organized and consistent in its structure, but there is a minor inconsistency in the use of 'shall' and 'should' in the functional and non-functional requirements.\"\n",
            "    },\n",
            "    \"Verifiability & Testability\": {\n",
            "      \"score\": 7,\n",
            "      \"reason\": \"Most requirements can be objectively verified or tested, but some, like NFR1, are too vague to be tested directly.\"\n",
            "    },\n",
            "    \"Structure & Standard Compliance\": {\n",
            "      \"score\": 5,\n",
            "      \"reason\": \"The SRS does not follow the standard IEEE 830/IEEE 29148 format, and some sections (e.g., Assumptions) are not clearly defined.\"\n",
            "    },\n",
            "    \"Overall Score\": 6.8\n",
            "  },\n",
            "  \"code_generation_evaluation\": {\n",
            "    \"Functional Correctness\": {\n",
            "      \"score\": 8,\n",
            "      \"reason\": \"The code correctly implements the required functionality, but it does not handle the case where the number of copies is set to a negative value when adding a book.\"\n",
            "    },\n",
            "    \"Compilation / Execution Validity\": {\n",
            "      \"score\": 10,\n",
            "      \"reason\": \"The code is free from syntax and runtime errors.\"\n",
            "    },\n",
            "    \"Algorithmic Efficiency\": {\n",
            "      \"score\": 6,\n",
            "      \"reason\": \"The time complexity of the code is O(n) for the listBooks method, which is acceptable for a small library. However, for a large library, this could be improved by using a data structure that allows for faster lookup, such as a HashSet or a database.\"\n",
            "    },\n",
            "    \"Readability & Maintainability\": {\n",
            "      \"score\": 9,\n",
            "      \"reason\": \"The code is well-structured and easy to read, but it could benefit from more comments and documentation to explain the purpose of each method and the logic behind the code.\"\n",
            "    },\n",
            "    \"Edge Case Handling\": {\n",
            "      \"score\": 7,\n",
            "      \"reason\": \"The code correctly handles the case where a book with the same ID already exists, but it does not handle the case where the number of copies is set to a negative value when adding a book.\"\n",
            "    },\n",
            "    \"Overall Score\": 7.6\n",
            "  },\n",
            "  \"test_cases_evaluation\": {\n",
            "    \"Test Case Coverage\": {\n",
            "      \"score\": 8,\n",
            "      \"reason\": \"The test cases cover most normal and boundary input scenarios, but lack extreme input scenarios, such as adding a book with a negative number of copies or issuing a book with a non-existent ID.\"\n",
            "    },\n",
            "    \"Edge Case Effectiveness\": {\n",
            "      \"score\": 6,\n",
            "      \"reason\": \"The test cases include some meaningful edge cases, such as adding a duplicate book and issuing a book when no copies are available, but miss others, like returning a book with no copies available.\"\n",
            "    },\n",
            "    \"Input Validity & Constraints Compliance\": {\n",
            "      \"score\": 9,\n",
            "      \"reason\": \"The test inputs are generally valid and well-formed, but the input for Test Case 4 (issueBook(999)) is not compliant with the problem constraints, as the book ID is not present in the system.\"\n",
            "    },\n",
            "    \"Expected Output Correctness\": {\n",
            "      \"score\": 9,\n",
            "      \"reason\": \"The expected outputs are mostly accurate and logically consistent with the problem specification, but Test Case 4's expected output is incorrect, as the book is not found instead of no copies available.\"\n",
            "    },\n",
            "    \"Redundancy & Diversity\": {\n",
            "      \"score\": 7,\n",
            "      \"reason\": \"The test cases avoid unnecessary duplication, but some test cases, like Test Case 1 and Test Case 2, test similar scenarios, and the test cases could benefit from more diversity to test different execution paths.\"\n",
            "    },\n",
            "    \"Overall Score\": 7.6\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_json_from_string(text):\n",
        "    # 1. Try direct JSON parsing (VERY IMPORTANT)\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2. Try JSON inside ```json ``` block\n",
        "    match = re.search(r'```json\\s*({[\\s\\S]*?})\\s*```', text)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    # 3. Try JSON inside generic ``` ``` block\n",
        "    match = re.search(r'```\\s*({[\\s\\S]*?})\\s*```', text)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "'''def extract_srs_evaluation(text):\n",
        "    \"\"\"\n",
        "    Extracts SRS evaluation.\n",
        "    - If JSON is present → return parsed JSON (dict)\n",
        "    - Else → return clean plain text\n",
        "    \"\"\"\n",
        "    # Try JSON extraction first\n",
        "    json_data = extract_json_from_string(text)\n",
        "    if json_data:\n",
        "        return json_data\n",
        "\n",
        "    # Fallback to plain text\n",
        "    return text.strip()\n",
        "'''\n",
        "# ----------------------------\n",
        "# Correct extraction\n",
        "# ----------------------------\n",
        "\n",
        "srs_evaluation = extract_json_from_string(raw_output_srs) or raw_output_srs.strip()\n",
        "cg_json = extract_json_from_string(raw_output_cg)\n",
        "tc_json = extract_json_from_string(raw_output_tc)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Combined output (sequence intact)\n",
        "# ----------------------------\n",
        "\n",
        "combined_evaluations = {\n",
        "    \"srs_evaluation\": srs_evaluation,\n",
        "    \"code_generation_evaluation\": cg_json,\n",
        "    \"test_cases_evaluation\": tc_json\n",
        "}\n",
        "\n",
        "output_filename = \"combined_evaluations.json\"\n",
        "\n",
        "with open(output_filename, \"w\") as f:\n",
        "    json.dump(combined_evaluations, f, indent=2)\n",
        "\n",
        "print(f\"Combined evaluations saved to {output_filename}\")\n",
        "\n",
        "# Display the content of the combined file\n",
        "with open(output_filename, \"r\") as f:\n",
        "    print(\"\\nContent of combined_evaluations.json:\\n\")\n",
        "    print(f.read())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49c29b66",
        "outputId": "d0d98f64-533e-4764-f7d7-fcfcb0791840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted text output saved to formatted_evaluations.txt\n",
            "\n",
            "Content of formatted_evaluations.txt:\n",
            "\n",
            "SRS Evaluation\n",
            "\n",
            "Requirements Completeness\n",
            "  Score: 6/10\n",
            "  Reason: The SRS covers the main functional and non-functional requirements, but lacks details on book management, user authentication, and error handling.\n",
            "\n",
            "Clarity & Unambiguity\n",
            "  Score: 8/10\n",
            "  Reason: The requirements are generally clear and concise, but some terms like 'unique Book ID' could be clarified.\n",
            "\n",
            "Consistency\n",
            "  Score: 9/10\n",
            "  Reason: The SRS is well-organized and consistent in its structure, but there is a minor inconsistency in the use of 'shall' and 'should' in the functional and non-functional requirements.\n",
            "\n",
            "Verifiability & Testability\n",
            "  Score: 7/10\n",
            "  Reason: Most requirements can be objectively verified or tested, but some, like NFR1, are too vague to be tested directly.\n",
            "\n",
            "Structure & Standard Compliance\n",
            "  Score: 5/10\n",
            "  Reason: The SRS does not follow the standard IEEE 830/IEEE 29148 format, and some sections (e.g., Assumptions) are not clearly defined.\n",
            "\n",
            "Overall Score: 6.8\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Code Generation Evaluation\n",
            "\n",
            "Functional Correctness\n",
            "  Score: 8/10\n",
            "  Reason: The code correctly implements the required functionality, but it does not handle the case where the number of copies is set to a negative value when adding a book.\n",
            "\n",
            "Compilation / Execution Validity\n",
            "  Score: 10/10\n",
            "  Reason: The code is free from syntax and runtime errors.\n",
            "\n",
            "Algorithmic Efficiency\n",
            "  Score: 6/10\n",
            "  Reason: The time complexity of the code is O(n) for the listBooks method, which is acceptable for a small library. However, for a large library, this could be improved by using a data structure that allows for faster lookup, such as a HashSet or a database.\n",
            "\n",
            "Readability & Maintainability\n",
            "  Score: 9/10\n",
            "  Reason: The code is well-structured and easy to read, but it could benefit from more comments and documentation to explain the purpose of each method and the logic behind the code.\n",
            "\n",
            "Edge Case Handling\n",
            "  Score: 7/10\n",
            "  Reason: The code correctly handles the case where a book with the same ID already exists, but it does not handle the case where the number of copies is set to a negative value when adding a book.\n",
            "\n",
            "Overall Score: 7.6\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Cases Evaluation\n",
            "\n",
            "Test Case Coverage\n",
            "  Score: 8/10\n",
            "  Reason: The test cases cover most normal and boundary input scenarios, but lack extreme input scenarios, such as adding a book with a negative number of copies or issuing a book with a non-existent ID.\n",
            "\n",
            "Edge Case Effectiveness\n",
            "  Score: 6/10\n",
            "  Reason: The test cases include some meaningful edge cases, such as adding a duplicate book and issuing a book when no copies are available, but miss others, like returning a book with no copies available.\n",
            "\n",
            "Input Validity & Constraints Compliance\n",
            "  Score: 9/10\n",
            "  Reason: The test inputs are generally valid and well-formed, but the input for Test Case 4 (issueBook(999)) is not compliant with the problem constraints, as the book ID is not present in the system.\n",
            "\n",
            "Expected Output Correctness\n",
            "  Score: 9/10\n",
            "  Reason: The expected outputs are mostly accurate and logically consistent with the problem specification, but Test Case 4's expected output is incorrect, as the book is not found instead of no copies available.\n",
            "\n",
            "Redundancy & Diversity\n",
            "  Score: 7/10\n",
            "  Reason: The test cases avoid unnecessary duplication, but some test cases, like Test Case 1 and Test Case 2, test similar scenarios, and the test cases could benefit from more diversity to test different execution paths.\n",
            "\n",
            "Overall Score: 7.6\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def format_evaluation_to_text(eval_data, section_title):\n",
        "    text_output = f\"{section_title} Evaluation\\n\\n\"\n",
        "    if eval_data:\n",
        "        for metric_name, details in eval_data.items():\n",
        "            if metric_name == \"Overall Score\":\n",
        "                continue\n",
        "            score = details.get(\"score\", \"N/A\")\n",
        "            reason = details.get(\"reason\", \"No reason provided\")\n",
        "            text_output += f\"{metric_name}\\n\"\n",
        "            text_output += f\"  Score: {score}/10\\n\"\n",
        "            text_output += f\"  Reason: {reason}\\n\\n\"\n",
        "        overall_score = eval_data.get(\"Overall Score\", \"N/A\")\n",
        "        text_output += f\"Overall Score: {overall_score}\\n\"\n",
        "    else:\n",
        "        text_output += \"No evaluation data available for this section.\\n\"\n",
        "    return text_output\n",
        "\n",
        "# Load the combined_evaluations.json if not already loaded\n",
        "# This assumes combined_evaluations variable exists from previous execution\n",
        "# If it doesn't, you might need to re-run the previous cell or load it from file\n",
        "\n",
        "formatted_text_output = \"\"\n",
        "\n",
        "formatted_text_output += format_evaluation_to_text(combined_evaluations.get(\"srs_evaluation\"), \"SRS\")\n",
        "formatted_text_output += \"\\n\" + \"-\"*50 + \"\\n\\n\" # Separator\n",
        "formatted_text_output += format_evaluation_to_text(combined_evaluations.get(\"code_generation_evaluation\"), \"Code Generation\")\n",
        "formatted_text_output += \"\\n\" + \"-\"*50 + \"\\n\\n\" # Separator\n",
        "formatted_text_output += format_evaluation_to_text(combined_evaluations.get(\"test_cases_evaluation\"), \"Test Cases\")\n",
        "\n",
        "output_text_filename = \"formatted_evaluations.txt\"\n",
        "with open(output_text_filename, \"w\") as f:\n",
        "    f.write(formatted_text_output)\n",
        "\n",
        "print(f\"Formatted text output saved to {output_text_filename}\")\n",
        "\n",
        "# Display the content of the formatted text file\n",
        "print(\"\\nContent of formatted_evaluations.txt:\\n\")\n",
        "print(formatted_text_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X88awdJdrB0",
        "outputId": "0585ae8c-f609-4692-d949-582c65a71212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for cloudflared (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit cloudflared pdfplumber\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlfcMtnuPGoA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\".streamlit\", exist_ok=True)\n",
        "\n",
        "with open(\".streamlit/secrets.toml\", \"w\") as f:\n",
        "    f.write('OPENROUTER_API_KEY = \"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\"\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrZo3te0Q0l-",
        "outputId": "c8a314ca-0ef5-4d53-ac6b-ad1e3b6ec0cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENROUTER_API_KEY = \"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\"\n"
          ]
        }
      ],
      "source": [
        "!cat .streamlit/secrets.toml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb5cnueRddEz",
        "outputId": "d8b829e9-bb4f-4c18-efe9-47dc398f7dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import streamlit as st\n",
        "\n",
        "# =====================================================\n",
        "# ---------------- LLM Client Setup -------------------\n",
        "# =====================================================\n",
        "\n",
        "# Option 1 (Recommended): Set in Colab\n",
        "OPENROUTER_API_KEY = st.secrets[\"OPENROUTER_API_KEY\"]\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    raise RuntimeError(\n",
        "        \"OPENROUTER_API_KEY is not set. \"\n",
        "        \"Please set it using os.environ or Streamlit secrets.\"\n",
        "    )\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import json\n",
        "import re\n",
        "import pdfplumber\n",
        "\n",
        "# =====================================================\n",
        "# ---------------- Utility Functions ------------------\n",
        "# =====================================================\n",
        "\n",
        "def extract_text_from_pdf(uploaded_file):\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp:\n",
        "        tmp.write(uploaded_file.read())\n",
        "        pdf_path = tmp.name\n",
        "\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def extract_text_file(uploaded_file):\n",
        "    return uploaded_file.read().decode(\"utf-8\").strip()\n",
        "\n",
        "\n",
        "def extract_json_from_string(text):\n",
        "    \"\"\"\n",
        "    Robust JSON extractor:\n",
        "    1. Raw JSON\n",
        "    2. ```json { } ```\n",
        "    3. ``` { } ```\n",
        "    \"\"\"\n",
        "    # Case 1: raw JSON\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Case 2: fenced ```json\n",
        "    match = re.search(r'```json\\s*({[\\s\\S]*?})\\s*```', text)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Case 3: fenced ```\n",
        "    match = re.search(r'```\\s*({[\\s\\S]*?})\\s*```', text)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(1))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_srs_evaluation(text):\n",
        "    \"\"\"\n",
        "    SRS may be valid JSON or plain text.\n",
        "    \"\"\"\n",
        "    json_data = extract_json_from_string(text)\n",
        "    if json_data:\n",
        "        return json_data\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def dict_to_readable_text(title, data):\n",
        "    \"\"\"\n",
        "    Converts evaluation JSON/dict into clean, readable text.\n",
        "    \"\"\"\n",
        "    lines = [f\"{title}\\n\" + \"=\" * len(title)]\n",
        "\n",
        "    if not isinstance(data, dict):\n",
        "        lines.append(str(data))\n",
        "        return \"\\n\".join(lines)\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if isinstance(value, dict):\n",
        "            lines.append(f\"\\n{key}:\")\n",
        "            for sub_key, sub_val in value.items():\n",
        "                lines.append(f\"  {sub_key}: {sub_val}\")\n",
        "        else:\n",
        "            lines.append(f\"\\n{key}: {value}\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ----------------   SRS Evaluation  ------------------\n",
        "# =====================================================\n",
        "\n",
        "SRS_METRICS = [\n",
        "    {\n",
        "        \"name\": \"Requirements Completeness\",\n",
        "        \"description\": (\n",
        "            \"Does the SRS comprehensively cover all necessary functional and non-functional \"\n",
        "            \"requirements, including system features, constraints, assumptions, and dependencies?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Clarity & Unambiguity\",\n",
        "        \"description\": (\n",
        "            \"Are the requirements clearly stated, precise, and free from ambiguity, vague terms, \"\n",
        "            \"or subjective language that could lead to multiple interpretations?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Consistency\",\n",
        "        \"description\": (\n",
        "            \"Are there any conflicting, duplicated, or contradictory requirements within the SRS, \"\n",
        "            \"or between functional and non-functional sections?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Verifiability & Testability\",\n",
        "        \"description\": (\n",
        "            \"Can each requirement be objectively verified or tested through inspection, analysis, \"\n",
        "            \"demonstration, or test cases?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Structure & Standard Compliance\",\n",
        "        \"description\": (\n",
        "            \"Is the SRS well-structured and logically organized, following standard SRS formats \"\n",
        "            \"(e.g., IEEE 830 / IEEE 29148) with proper sections and hierarchy?\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "def build_geval_prompt_srs_pdf(srs_pdf_text, metrics):\n",
        "    metric_text = \"\\n\".join([\n",
        "        f\"{i+1}. {m['name']}: {m['description']}\"\n",
        "        for i, m in enumerate(metrics)\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "You are an expert software requirements analyst and SRS reviewer.\n",
        "\n",
        "Evaluate the following Software Requirements Specification (SRS) document\n",
        "generated by a Large Language Model. The SRS content has been extracted\n",
        "from a PDF file.\n",
        "\n",
        "SRS DOCUMENT (Extracted from PDF):\n",
        "{srs_pdf_text}\n",
        "\n",
        "EVALUATION METRICS:\n",
        "{metric_text}\n",
        "\n",
        "For each metric:\n",
        "- Assign a score from 1 to 10 (higher is better).\n",
        "- Provide a concise and technically sound justification.\n",
        "- Evaluate strictly based on SRS best practices and standards\n",
        "  (e.g., IEEE 830 / IEEE 29148).\n",
        "\n",
        "Return STRICTLY in valid JSON format.\n",
        "Do NOT include markdown, explanations, or additional text.\n",
        "\n",
        "Expected JSON format:\n",
        "{{\n",
        "  \"Metric Name\": {{\n",
        "      \"score\": <int>,\n",
        "      \"reason\": \"<short justification>\"\n",
        "  }},\n",
        "  ...\n",
        "  \"Overall Score\": <float>\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "def run_geval_srs(prompt):\n",
        "    \"\"\"\n",
        "    Executes GEVAL evaluation for an SRS document (text extracted from PDF).\n",
        "\n",
        "    Args:\n",
        "        prompt (str): Fully constructed GEVAL prompt for SRS evaluation.\n",
        "\n",
        "    Returns:\n",
        "        str: Raw JSON response from the LLM evaluator.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=900\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# =====================================================\n",
        "# ---------------- Code Generation --------------------\n",
        "# =====================================================\n",
        "\n",
        "CG_METRICS = [\n",
        "    {\n",
        "        \"name\": \"Functional Correctness\",\n",
        "        \"description\": \"Does the code correctly solve the problem for all valid inputs and edge cases?\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Compilation / Execution Validity\",\n",
        "        \"description\": \"Is the code free from syntax and runtime errors?\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Algorithmic Efficiency\",\n",
        "        \"description\": \"Is the time and space complexity appropriate for the problem constraints?\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Readability & Maintainability\",\n",
        "        \"description\": \"Is the code clean, readable, and well-structured?\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Edge Case Handling\",\n",
        "        \"description\": \"Does the code correctly handle boundary and corner cases?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def build_geval_prompt_cg(problem, code, metrics):\n",
        "    metric_text = \"\\n\".join([\n",
        "        f\"{i+1}. {m['name']}: {m['description']}\"\n",
        "        for i, m in enumerate(metrics)\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "You are an expert software engineer and code reviewer.\n",
        "\n",
        "Evaluate the following generated code using the metrics below.\n",
        "\n",
        "PROBLEM DESCRIPTION:\n",
        "{problem}\n",
        "\n",
        "GENERATED CODE:\n",
        "{code}\n",
        "\n",
        "EVALUATION METRICS:\n",
        "{metric_text}\n",
        "\n",
        "For each metric:\n",
        "- Give a score (1-10) and short reason for each metric.\n",
        "- Return ONLY valid JSON in this format:\n",
        "\n",
        "Return STRICTLY in valid JSON format:\n",
        "{{\n",
        "  \"Metric Name\": {{\n",
        "      \"score\": <int>,\n",
        "      \"reason\": \"<text>\"\n",
        "  }},\n",
        "  ...\n",
        "  \"Overall Score\": <float>\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "def run_geval_cg(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=700\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ---------------- Testcase Evaluation ----------------\n",
        "# =====================================================\n",
        "\n",
        "TESTCASE_METRICS = [\n",
        "    {\n",
        "        \"name\": \"Test Case Coverage\",\n",
        "        \"description\": (\n",
        "            \"Do the generated test cases adequately cover normal, boundary, and extreme input scenarios \"\n",
        "            \"defined by the problem constraints?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Edge Case Effectiveness\",\n",
        "        \"description\": (\n",
        "            \"Do the test cases include meaningful edge and corner cases that are likely to expose defects \"\n",
        "            \"in incorrect or incomplete implementations?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Input Validity & Constraints Compliance\",\n",
        "        \"description\": (\n",
        "            \"Are all test inputs valid, well-formed, and compliant with the stated input constraints \"\n",
        "            \"(data types, ranges, formats, and sizes)?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Expected Output Correctness\",\n",
        "        \"description\": (\n",
        "            \"Are the expected outputs accurate and logically consistent with the problem specification \"\n",
        "            \"for each generated test case?\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Redundancy & Diversity\",\n",
        "        \"description\": (\n",
        "            \"Do the test cases avoid unnecessary duplication while maintaining sufficient diversity \"\n",
        "            \"to test different execution paths?\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "def build_geval_prompt_testcases(problem, code, testcases, metrics):\n",
        "    metric_text = \"\\n\".join([\n",
        "        f\"{i+1}. {m['name']}: {m['description']}\"\n",
        "        for i, m in enumerate(metrics)\n",
        "    ])\n",
        "\n",
        "    return f\"\"\"\n",
        "You are an expert software testing engineer and quality analyst.\n",
        "\n",
        "Your task is to evaluate the quality of test cases generated by a Large Language Model (LLM).\n",
        "The goal is to assess whether these test cases are effective at validating the correctness\n",
        "and robustness of the given code with respect to the problem description.\n",
        "\n",
        "PROBLEM DESCRIPTION:\n",
        "{problem}\n",
        "\n",
        "REFERENCE CODE UNDER TEST:\n",
        "{code}\n",
        "\n",
        "GENERATED TEST CASES:\n",
        "{testcases}\n",
        "\n",
        "EVALUATION METRICS:\n",
        "{metric_text}\n",
        "\n",
        "For each metric:\n",
        "- Assign a score between 1 and 10\n",
        "- Provide a concise, technical justification focused on test effectiveness\n",
        "\n",
        "Return STRICTLY valid JSON in the following format:\n",
        "{{\n",
        "  \"Metric Name\": {{\n",
        "      \"score\": <int>,\n",
        "      \"reason\": \"<concise explanation>\"\n",
        "  }},\n",
        "  ...\n",
        "  \"Overall Score\": <float>\n",
        "}}\n",
        "\n",
        "Important:\n",
        "- Do NOT evaluate code style or performance\n",
        "- Focus ONLY on test coverage, correctness, diversity, and defect-detection capability\n",
        "- Penalize redundant, invalid, or weak test cases\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def run_testcase_geval(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/llama-3.1-8b-instruct\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=700\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ---------------- Streamlit UI -----------------------\n",
        "# =====================================================\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"GEVAL – Evaluation System\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "st.title(\"GEVAL – SRS Evaluation System\")\n",
        "st.caption(\"Streamlit app running on Google Colab\")\n",
        "\n",
        "# ---------------- DEBUG (safe to keep) ----------------\n",
        "st.write(\"UI loaded successfully.\")\n",
        "\n",
        "# =====================================================\n",
        "# ---------------- File Upload Section ----------------\n",
        "# =====================================================\n",
        "\n",
        "st.header(\"Upload Inputs\")\n",
        "\n",
        "srs_pdf = st.file_uploader(\n",
        "    \"Upload SRS Document (PDF)\",\n",
        "    type=[\"pdf\"]\n",
        ")\n",
        "\n",
        "problem_file = st.file_uploader(\n",
        "    \"Upload Problem Description (.txt)\",\n",
        "    type=[\"txt\"]\n",
        ")\n",
        "\n",
        "code_file = st.file_uploader(\n",
        "    \"Upload Generated Code (.java / .py / .cpp / .txt)\",\n",
        "    type=[\"java\", \"py\", \"cpp\", \"txt\"]\n",
        ")\n",
        "\n",
        "testcase_file = st.file_uploader(\n",
        "    \"Upload Test Cases (.txt / .json)\",\n",
        "    type=[\"txt\", \"json\"]\n",
        ")\n",
        "\n",
        "run_eval = st.button(\"Run Evaluation\")\n",
        "\n",
        "# =====================================================\n",
        "# ---------------- Evaluation Pipeline ----------------\n",
        "# =====================================================\n",
        "\n",
        "if run_eval:\n",
        "    combined_evaluations = {}\n",
        "\n",
        "    # ---------------- SRS Evaluation ----------------\n",
        "    if srs_pdf:\n",
        "        with st.spinner(\"Evaluating SRS Document...\"):\n",
        "            srs_text = extract_text_from_pdf(srs_pdf)\n",
        "            prompt_srs = build_geval_prompt_srs_pdf(\n",
        "                srs_text,\n",
        "                SRS_METRICS\n",
        "            )\n",
        "            raw_output_srs = run_geval_srs(prompt_srs)\n",
        "            srs_eval = extract_srs_evaluation(raw_output_srs)\n",
        "\n",
        "        st.subheader(\"SRS Evaluation Report\")\n",
        "        if isinstance(srs_eval, dict):\n",
        "            srs_text_output = dict_to_readable_text(\"SRS Evaluation\", srs_eval)\n",
        "            st.text_area(\"SRS Evaluation Report\", srs_text_output, height=400)\n",
        "        else:\n",
        "            st.text_area(\"SRS Evaluation\", srs_eval, height=350)\n",
        "\n",
        "        combined_evaluations[\"srs_evaluation\"] = srs_eval\n",
        "    else:\n",
        "        st.warning(\"SRS PDF not uploaded.\")\n",
        "\n",
        "    # ---------------- Code Generation Evaluation ----------------\n",
        "    if problem_file and code_file:\n",
        "        with st.spinner(\"Evaluating Generated Code...\"):\n",
        "            problem_text = extract_text_file(problem_file)\n",
        "            code_text = extract_text_file(code_file)\n",
        "\n",
        "            prompt_code = build_geval_prompt_cg(\n",
        "                problem_text,\n",
        "                code_text,\n",
        "                CG_METRICS\n",
        "            )\n",
        "            raw_output_cg = run_geval_cg(prompt_code)\n",
        "            cg_eval = extract_json_from_string(raw_output_cg)\n",
        "\n",
        "        st.subheader(\"Code Generation Evaluation Report\")\n",
        "        if cg_eval:\n",
        "            cg_text_output = dict_to_readable_text(\"Code Generation Evaluation\", cg_eval)\n",
        "            st.text_area(\"Code Generation Evaluation Report\", cg_text_output, height=400)\n",
        "        else:\n",
        "            st.warning(\"Invalid JSON returned for Code Evaluation.\")\n",
        "            st.text(raw_output_cg)\n",
        "\n",
        "        combined_evaluations[\"code_generation_evaluation\"] = cg_eval\n",
        "    else:\n",
        "        st.warning(\"Problem description or code file missing.\")\n",
        "\n",
        "    # ---------------- Test Case Evaluation ----------------\n",
        "    if testcase_file:\n",
        "        with st.spinner(\"Evaluating Test Cases...\"):\n",
        "            tc_text = extract_text_file(testcase_file)\n",
        "            prompt_tc = build_geval_prompt_testcases(\n",
        "                problem_text,\n",
        "                code_text,\n",
        "                tc_text,\n",
        "                TESTCASE_METRICS\n",
        "            )\n",
        "            raw_output_tc = run_testcase_geval(prompt_tc)\n",
        "            tc_eval = extract_json_from_string(raw_output_tc)\n",
        "\n",
        "        st.subheader(\"Test Case Evaluation Report\")\n",
        "        if tc_eval:\n",
        "            tc_text_output = dict_to_readable_text(\"Test Case Evaluation\", tc_eval)\n",
        "            st.text_area(\"Test Case Evaluation Report\", tc_text_output, height=400)\n",
        "        else:\n",
        "            st.warning(\"Invalid JSON returned for Test Case Evaluation.\")\n",
        "            st.text(raw_output_tc)\n",
        "\n",
        "        combined_evaluations[\"test_cases_evaluation\"] = tc_eval\n",
        "    else:\n",
        "        st.warning(\"Test case file not uploaded.\")\n",
        "\n",
        "    # ---------------- Save Combined Output ----------------\n",
        "    if combined_evaluations:\n",
        "        final_text_output = []\n",
        "\n",
        "        if \"srs_evaluation\" in combined_evaluations:\n",
        "          final_text_output.append(\n",
        "          dict_to_readable_text(\"SRS Evaluation\", combined_evaluations[\"srs_evaluation\"])\n",
        "          )\n",
        "\n",
        "        if \"code_generation_evaluation\" in combined_evaluations:\n",
        "          final_text_output.append(\n",
        "          dict_to_readable_text(\"Code Generation Evaluation\", combined_evaluations[\"code_generation_evaluation\"])\n",
        "          )\n",
        "\n",
        "        if \"test_cases_evaluation\" in combined_evaluations:\n",
        "          final_text_output.append(\n",
        "          dict_to_readable_text(\"Test Case Evaluation\", combined_evaluations[\"test_cases_evaluation\"])\n",
        "          )\n",
        "\n",
        "        final_text = \"\\n\\n\" + (\"\\n\" + \"=\" * 80 + \"\\n\\n\").join(final_text_output)\n",
        "\n",
        "        with open(\"combined_evaluations.txt\", \"w\") as f:\n",
        "          f.write(final_text)\n",
        "\n",
        "\n",
        "        st.success(\"Combined evaluations saved successfully.\")\n",
        "\n",
        "        st.download_button(\n",
        "          label=\"Download Evaluation Report (TXT)\",\n",
        "          data=final_text,\n",
        "          file_name=\"combined_evaluations.txt\",\n",
        "          mime=\"text/plain\"\n",
        "        )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBwQpARFgPtV"
      },
      "outputs": [],
      "source": [
        "!pkill -f streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lrLXRDQgRKg"
      },
      "outputs": [],
      "source": [
        "!nohup streamlit run app.py \\\n",
        "  --server.port 8501 \\\n",
        "  --server.address 0.0.0.0 \\\n",
        "  > streamlit.log 2>&1 &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc0F6scAgT0Z",
        "outputId": "7362f68e-ad78-4278-d994-2ee700472b00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root        1695  0.0  0.0  20960 12960 ?        R    13:33   0:00 /usr/bin/python3 /usr/local/bin/streamlit run app.py --server.port 8501 --server.address 0.0.0.0\n",
            "root        1696  0.0  0.0   7372  3404 ?        S    13:33   0:00 /bin/bash -c ps aux | grep streamlit\n",
            "root        1698  0.0  0.0   6480  2376 ?        S    13:33   0:00 grep streamlit\n"
          ]
        }
      ],
      "source": [
        "!ps aux | grep streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Efr-dq1b0uIi"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys_AKIS00vRM",
        "outputId": "afcc55da-cf34-4705-c5c5-9db275e4754e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cloudflared version 2025.11.1 (built 2025-11-07-16:59 UTC)\n"
          ]
        }
      ],
      "source": [
        "!cloudflared --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iycVrOMqDuLW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAsrcGBHgpQc",
        "outputId": "1935c46a-15cf-493d-cefd-ea654d1d11e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[90m2026-01-10T13:33:11Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2026-01-10T13:33:11Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m |  https://filters-telescope-district-point.trycloudflare.com                                |\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 460f1032-e90c-434b-99f9-93468b83a35a\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2026-01-10T13:33:14Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.43\n",
            "2026/01/10 13:33:14 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2026-01-10T13:33:15Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0mf4565412-aa10-41b6-a40c-5a3831274c08 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.200.43 \u001b[36mlocation=\u001b[0mord08 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        }
      ],
      "source": [
        "!cloudflared tunnel --url http://localhost:8501\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQeuOgQ95Bec"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}